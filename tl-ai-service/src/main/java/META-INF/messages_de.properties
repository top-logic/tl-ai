com.top_logic.ai.service.ChatModelFactory = Chat-Modellfabrik
com.top_logic.ai.service.ChatModelFactory.Config.max-idle-models = Maximale Modelle im Leerlauf
com.top_logic.ai.service.ChatModelFactory.Config.max-idle-models.tooltip = Die maximale Anzahl der inaktiven Chat-Modelle, die im Pool gehalten werden. <p> Voreinstellung: 5 </p>
com.top_logic.ai.service.ChatModelFactory.Config.max-pool-size = Maximale Poolgröße
com.top_logic.ai.service.ChatModelFactory.Config.max-pool-size.tooltip = Die maximale Anzahl von Chatmodellen im Pool. <p> Standard: 10 </p>
com.top_logic.ai.service.ChatModelFactory.Config.models = Modelle
com.top_logic.ai.service.ChatModelFactory.Config.models.tooltip = Die Modellnamen (z. B. "gpt-4o, gpt-3.5-turbo" oder "claude-3-opus, claude-3-sonnet"). <p> Kommagetrennte Liste von Modellnamen. Jeder Modellname dient als eindeutiger Bezeichner und bestimmt, welche Modelle angefordert werden können. Alle aufgelisteten Modelle verwenden dieselben API-Anmeldeinformationen von dieser Fabrik. </p>
com.top_logic.ai.service.ChatModelFactory.tooltip = Abstrakte Fabrik zum Erstellen und Verwalten von gepoolten <i>Chat-Modell-Instanzen</i>. <p> Diese Fabrik erstellt Objektpools für <i>LangChain4j-Chatmodell-Instanzen</i>. Jede Fabrik ist für einen bestimmten AI-Anbieter konfiguriert und kann Pools für mehrere Modelle dieses Anbieters erstellen. </p> <p> Konkrete Implementierungen existieren für verschiedene AI-Anbieter: </p> <ul> <li><i>OpenAI-Chatmodellfabrik</i>: Für OpenAI-Modelle (GPT-4o, GPT-3.5-turbo, etc.)</li> <li><i>anthropische Chat-Modell-Fabrik</i>: Für anthropische Modelle (Claude, etc.)</li> <li><i>mistral chat model factory</i>: Für Mistral-KI-Modelle</li> </ul>
com.top_logic.ai.service.OpenAIService = AI-Dienst
com.top_logic.ai.service.OpenAIService.Config.default-model = Standardmodell
com.top_logic.ai.service.OpenAIService.Config.default-model.tooltip = Der Standardmodellname, der verwendet wird, wenn kein Modell explizit angegeben wird. <p> Dieser sollte mit einem der in <i>Factories</i> konfigurierten Modellnamen übereinstimmen. Wird er nicht angegeben oder ist er null, wird das Modell der ersten Fabrik als Standard verwendet. </p>
com.top_logic.ai.service.OpenAIService.Config.factories = Fabriken
com.top_logic.ai.service.OpenAIService.Config.factories.tooltip = Die konfigurierten Modellfabriken. <p> Jede Fabrik wird durch den Modellnamen identifiziert, den sie produziert, und verwaltet einen Pool von Chat-Modell-Instanzen für dieses spezifische Modell. </p>
com.top_logic.ai.service.OpenAIService.tooltip = TopLogic-Dienst, der den Zugriff auf KI-Modelle über <i>LangChain4j</i> mit thread-sicherem Client-Pooling ermöglicht. <p> Dieser Dienst verwaltet mehrere <i>Chat-Modell-Fabrik-Instanzen</i>, die jeweils für ein bestimmtes KI-Modell konfiguriert sind. Die Modelle werden gepoolt und automatisch an den aktuellen <i>Interaktionskontext</i> gebunden und dann an den Pool zurückgegeben, wenn der Kontext zerstört wird. </p>
com.top_logic.ai.service.providers.AnthropicChatModelFactory = Antropic Chat-Modellfabrik\ 
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.api-key = API-Schlüssel
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.api-key.tooltip = Der Anthropic-API-Schlüssel für die Authentifizierung.
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.base-url = Basis-URL
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.base-url.tooltip = Die Basis-URL für die Anthropic-API. <p> Standard: https://api.anthropic.com/v1 </p>
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.version = Version
com.top_logic.ai.service.providers.AnthropicChatModelFactory.Config.version.tooltip = Die zu verwendende API-Version. <p> Optional. Anthropic verwendet versionierte APIs. Wenn nicht angegeben, wird der Standardwert von <i>LangChain4j</i> verwendet. </p>
com.top_logic.ai.service.providers.AnthropicChatModelFactory.tooltip = Fabrik zur Erstellung von Anthropic-Chatmodell-Instanzen. <p> Diese Fabrik erstellt <i>Chatmodell-Instanzen</i> unter Verwendung der Anthropic-API für Claude-Modelle. </p>
com.top_logic.ai.service.providers.MistralChatModelFactory = Mistral Chat Modellfabrik
com.top_logic.ai.service.providers.MistralChatModelFactory.Config.api-key = API-Schlüssel
com.top_logic.ai.service.providers.MistralChatModelFactory.Config.api-key.tooltip = Der Mistral AI API Schlüssel für die Authentifizierung.
com.top_logic.ai.service.providers.MistralChatModelFactory.Config.base-url = Basis-URL
com.top_logic.ai.service.providers.MistralChatModelFactory.Config.base-url.tooltip = Die Basis-URL für die Mistral AI API. <p> Standard: https://api.mistral.ai/v1 </p>
com.top_logic.ai.service.providers.MistralChatModelFactory.tooltip = Fabrik zur Erstellung von Mistral AI Chat-Modell-Instanzen. <p> Diese Fabrik erstellt <i>Chat-Modell-Instanzen</i> unter Verwendung der Mistral AI API für Modelle wie Mistral Large, Mistral Medium, etc. </p>
com.top_logic.ai.service.providers.OpenAIChatModelFactory = OpenAI Chat-Modellfabrik
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.api-key = API-Schlüssel
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.api-key.tooltip = Der <i>OpenAI-API-Schlüssel</i> für die Authentifizierung.
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.base-url = Basis-URL
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.base-url.tooltip = Die Basis-URL für die <i>OpenAI-API</i>. <p> Standard: https://api.openai.com/v1 </p> <p> Sie können diesen Wert überschreiben, um einen anderen Endpunkt zu verwenden, z. B. einen Azure <i>OpenAI-Einsatz</i> oder einen lokalen Proxy-Server. </p>
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.organization = Organisation
com.top_logic.ai.service.providers.OpenAIChatModelFactory.Config.organization.tooltip = Die <i>OpenAI-Organisations-ID</i>. <p> Optional. Für Benutzer, die mehreren Organisationen angehören, können Sie angeben, welche Organisation für eine API-Anfrage verwendet wird. </p>
com.top_logic.ai.service.providers.OpenAIChatModelFactory.tooltip = Fabrik zur Erstellung von <i>OpenAI-Chatmodell-Instanzen</i>. <p> Diese Fabrik erstellt <i>Chatmodell-Instanzen</i> unter Verwendung der <i>OpenAI-API</i> (oder kompatibler APIs wie Azure <i>OpenAI</i>). </p>
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat = Chat
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat.param.messages.tooltip = Eine Liste von Nachrichten. Jede Nachricht kann sein: <ul> <li>Zeichenkette: Wird als Benutzertextnachricht behandelt</li> <li>Karte mit "Rolle" und "Inhalt": Explizite Rolle (System/Benutzer/Assistent). Der Inhalt kann eine Zeichenkette, binäre Daten oder eine Liste mit gemischtem Inhalt sein.</li> <li>Binäre Daten: Werden als Bild-/Dokumentennachricht des Benutzers behandelt</li> </ul>
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat.param.model.tooltip = Der zu verwendende Modellname (z. B. "gpt-4o", "gpt-3.5-turbo") oder null, um das im <i>offenen AI-Dienst</i> konfigurierte Standardmodell zu verwenden.
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat.param.responseFormat.tooltip = Die Spezifikation des Antwortformats. Kann sein: <ul> <li>Null: Keine spezifische Formatanforderung</li> <li>Zeichenfolge "text" oder "json": Einfacher Formattyp</li> <li>Karte mit den Feldern "name" und "schema", die ein JSON-Schema für eine strukturierte Ausgabe beschreiben.</li> </ul>
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat.return = Der Antworttext des Assistenten.
com.top_logic.ai.service.scripting.OpenAIScriptFunctions.chat.tooltip = Sendet eine Multiturn-Konversation an die Chat-Abschluss-API. <p> Nachrichten können Strings, Maps mit Rolle/Inhalt oder Listen sein. Der Inhalt kann aus Textstrings oder <i>binären Daten</i> für Bilder und Dokumente bestehen. </p>
