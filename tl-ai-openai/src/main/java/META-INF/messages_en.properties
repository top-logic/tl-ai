com.top_logic.ai.openai.ChatModelFactory = Chat model factory
com.top_logic.ai.openai.ChatModelFactory.Config.max-idle-models = Maximum idle models
com.top_logic.ai.openai.ChatModelFactory.Config.max-idle-models.tooltip = The maximum number of idle chat models kept in the pool. <p> Default: 5 </p>
com.top_logic.ai.openai.ChatModelFactory.Config.max-pool-size = Maximum pool size
com.top_logic.ai.openai.ChatModelFactory.Config.max-pool-size.tooltip = The maximum number of chat models in the pool. <p> Default: 10 </p>
com.top_logic.ai.openai.ChatModelFactory.Config.models = Models
com.top_logic.ai.openai.ChatModelFactory.Config.models.tooltip = The model names (e.g., "gpt-4o, gpt-3.5-turbo" or "claude-3-opus, claude-3-sonnet"). <p> This is a comma-separated list of model names. Each model name serves as a unique identifier and determines which models can be requested via <code>OpenAIService#getChatModel(String)</code>. All listed models will use the same API credentials from this factory. </p>
com.top_logic.ai.openai.ChatModelFactory.tooltip = Abstract factory for creating and managing pooled <i>chat model</i> instances. <p> This factory creates object pools for LangChain4j chat model instances. Each factory is configured for a specific AI provider and can create pools for multiple models from that provider. </p> <p> Concrete implementations exist for different AI providers: </p> <ul> <li><i>open AI chat model factory</i>: For OpenAI models (GPT-4o, GPT-3.5-turbo, etc.)</li> <li><i>anthropic chat model factory</i>: For Anthropic models (Claude, etc.)</li> <li><i>mistral chat model factory</i>: For Mistral AI models</li> </ul>
com.top_logic.ai.openai.OpenAIService = Open AI service
com.top_logic.ai.openai.OpenAIService.Config.default-model = Default model
com.top_logic.ai.openai.OpenAIService.Config.default-model.tooltip = The default model name to use when no model is explicitly specified. <p> This should match one of the model names configured in <i>Factories</i>. If not specified or null, the first factory's model will be used as default. </p>
com.top_logic.ai.openai.OpenAIService.Config.factories = Factories
com.top_logic.ai.openai.OpenAIService.Config.factories.tooltip = The configured model factories. <p> Each factory is identified by the model name it produces and manages a pool of chat model instances for that specific model. </p>
com.top_logic.ai.openai.OpenAIService.tooltip = TopLogic service that provides access to AI models via LangChain4j with thread-safe client pooling. <p> This service manages multiple <i>chat model factory</i> instances, each configured for a specific AI model. Models are pooled and automatically bound to the current <i>interaction context</i>, then returned to the pool when the context is destroyed. </p> <p> The service provides a singleton instance that can be accessed via <code>#getInstance()</code> and returns a <i>chat model</i> bound to the current thread context via <code>#getChatModel(String)</code>. </p>
com.top_logic.ai.openai.providers.AnthropicChatModelFactory = Anthropic chat model factory
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.api-key = Api key
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.api-key.tooltip = The Anthropic API key for authentication.
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.base-url = Base URL
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.base-url.tooltip = The base URL for the Anthropic API. <p> Default: https://api.anthropic.com/v1 </p>
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.version = Version
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.Config.version.tooltip = The API version to use. <p> Optional. Anthropic uses versioned APIs. If not specified, the LangChain4j default is used. </p>
com.top_logic.ai.openai.providers.AnthropicChatModelFactory.tooltip = Factory for creating Anthropic chat model instances. <p> This factory creates <i>chat model</i> instances using the Anthropic API for Claude models. </p>
com.top_logic.ai.openai.providers.MistralChatModelFactory = Mistral chat model factory
com.top_logic.ai.openai.providers.MistralChatModelFactory.Config.api-key = Api key
com.top_logic.ai.openai.providers.MistralChatModelFactory.Config.api-key.tooltip = The Mistral AI API key for authentication.
com.top_logic.ai.openai.providers.MistralChatModelFactory.Config.base-url = Base URL
com.top_logic.ai.openai.providers.MistralChatModelFactory.Config.base-url.tooltip = The base URL for the Mistral AI API. <p> Default: https://api.mistral.ai/v1 </p>
com.top_logic.ai.openai.providers.MistralChatModelFactory.tooltip = Factory for creating Mistral AI chat model instances. <p> This factory creates <i>chat model</i> instances using the Mistral AI API for models like Mistral Large, Mistral Medium, etc. </p>
com.top_logic.ai.openai.providers.OpenAIChatModelFactory = Open AI chat model factory
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.api-key = Api key
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.api-key.tooltip = The OpenAI API key for authentication.
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.base-url = Base URL
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.base-url.tooltip = The base URL for the OpenAI API. <p> Default: https://api.openai.com/v1 </p> <p> You can override this to use a different endpoint, such as an Azure OpenAI deployment or a local proxy server. </p>
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.organization = Organization
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.Config.organization.tooltip = The OpenAI organization ID. <p> Optional. For users who belong to multiple organizations, you can specify which organization is used for an API request. </p>
com.top_logic.ai.openai.providers.OpenAIChatModelFactory.tooltip = Factory for creating OpenAI chat model instances. <p> This factory creates <i>chat model</i> instances using the OpenAI API (or compatible APIs like Azure OpenAI). </p>
com.top_logic.ai.openai.scripting.OpenAIScriptFunctions.chat = Chat
com.top_logic.ai.openai.scripting.OpenAIScriptFunctions.chat.param.messages.tooltip = A list of messages. Each message can be: <ul> <li>String: Treated as user text message</li> <li>Map with "role" and "content": Explicit role (system/user/assistant). Content can be string, binary data, or a list of mixed content.</li> <li>Binary data: Treated as user image/document message</li> </ul>
com.top_logic.ai.openai.scripting.OpenAIScriptFunctions.chat.param.model.tooltip = The model name to use (e.g., "gpt-4o", "gpt-3.5-turbo"), or null to use the default model configured in <i>open AI service</i>.
com.top_logic.ai.openai.scripting.OpenAIScriptFunctions.chat.return = The assistant's response text.
com.top_logic.ai.openai.scripting.OpenAIScriptFunctions.chat.tooltip = Sends a multi-turn conversation to the chat completion API. <p> Messages can be strings, maps with role/content, or lists. Content can be text strings or <i>binary data</i> for images and documents. </p>
